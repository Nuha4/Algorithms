{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7dnBfpLtLlim17hWkRfv7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nuha4/Algorithms/blob/master/pilot_study_phishing_storis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkK5-MGlrMeD",
        "outputId": "f12c1ccc-d353-439b-d064-a031e4ae430e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Load and Inspect Data**"
      ],
      "metadata": {
        "id": "OAC8I9Z2roTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The name of the CSV file uploaded\n",
        "file_name = 'final_evaluation_data.csv'\n",
        "\n",
        "# Load the data into a pandas DataFrame\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# Display the first 5 rows to make sure it looks right\n",
        "print(\"First 5 rows of data:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "oocuzCA6rpLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate and Print Performance Metrics**\n"
      ],
      "metadata": {
        "id": "GAzqWqfgrvPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define \"true\" labels (Gold Standard) and the \"predicted\" labels (from the LLM)\n",
        "y_true = df['Human_Label_RQ2']\n",
        "y_pred = df['LLM_Label_RQ2']\n",
        "\n",
        "print(\"--- PERFORMANCE METRICS: LLM vs. HUMAN GOLD STANDARD ---\")\n",
        "\n",
        "# 1. Calculate and print the overall accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nOverall Accuracy: {accuracy:.2%}\\n\")\n",
        "print(\"This is the percentage of labels the LLM got exactly right.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# 2. Generate and print the detailed classification report\n",
        "print(\"\\nDetailed Classification Report (Precision, Recall, F1-Score):\\n\")\n",
        "# The 'zero_division=0' parameter prevents warnings if a label was never predicted by the LLM\n",
        "report = classification_report(y_true, y_pred, zero_division=0)\n",
        "print(report)\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "ovsgZAIMrtW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize the Errors with a Confusion Matrix**"
      ],
      "metadata": {
        "id": "SOU6o_Mhr1vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- VISUALIZING THE ERRORS: CONFUSION MATRIX ---\")\n",
        "\n",
        "# Get a sorted list of all unique labels that appear in data\n",
        "unique_labels = sorted(list(set(y_true) | set(y_pred)))\n",
        "\n",
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
        "\n",
        "# Plot the confusion matrix using seaborn for a nice visual\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=unique_labels, yticklabels=unique_labels)\n",
        "\n",
        "plt.title('Confusion Matrix: LLM Predictions vs. Human Gold Standard', fontsize=16)\n",
        "plt.ylabel('True Label (Gold Standard)', fontsize=12)\n",
        "plt.xlabel('Predicted Label (LLM Output)', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout() # Adjust plot to ensure everything fits without overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b8Y536qRsBSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: How to Interpret this Results\n",
        "\n",
        "\n",
        "1.  Overall Accuracy: This is headline number. \"The LLM achieved an overall accuracy of 85% on the pilot dataset.\"\n",
        "2.   Classification Report: This is where find the details.\n",
        "*   F1-Score: Look at the F1-score for each persona. A high F1-score (e.g., > 0.90) means the LLM is excellent at identifying that specific persona. A low score (e.g., < 0.70) means it struggles with that one.\n",
        "*   Precision vs. Recall: If precision is high but recall is low for \"Fake Courier,\" it means that when the LLM predicts \"Fake Courier,\" it's usually right, but it misses a lot of the actual \"Fake Courier\" stories (labeling them as something else).\n",
        "*   Support: This just tells how many examples of each persona were in the human-labeled data.\n",
        "\n",
        "3.  Confusion Matrix: This is diagnostic tool.\n",
        "*   The Diagonal: The numbers running from top-left to bottom-right are the correct predictions. Prefer these numbers to be high.\n",
        "*   Off-Diagonal Numbers: Any number not on the diagonal is an error. For example, if we find a \"3\" where the \"True Label\" row is Romance Scammer and the \"Predicted Label\" column is Family Impersonator, it means the LLM made that specific mistake 3 times. This tells exactly which personas the LLM is confusing with each other."
      ],
      "metadata": {
        "id": "Obd-sahBsHpz"
      }
    }
  ]
}